{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1513028,"sourceType":"datasetVersion","datasetId":891580},{"sourceId":34958802,"sourceType":"kernelVersion"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Multimedia Project: Emotion Detection from Speech\n\n###  Team Members:\n- Mariam Raafat  \n- Engy Refaai  \n- Alyaa Tamer  \n- Soher Mohamed\n\n---\n\n###  Project Highlights:\n\n-  Preprocessed emotional speech audio signals\n-  Extracted features (MFCCs, pitch, energy)\n-  Visualized waveform and frequency changes\n-  Trained a neural network (MLP) to classify emotions\n\n>  This project explores how signal processing and machine learning can be combined to detect human emotions from audio using the **EMO-DB** dataset (Berlin Database of Emotional Speech).\n","metadata":{}},{"cell_type":"markdown","source":"##  Introduction\n\nIn this project, we classify human emotions from speech using audio processing techniques (like pitch shifting, noise reduction, and time stretching) and then use a neural network to detect emotions.\n\nThe dataset used is EMO-DB: Berlin Database of Emotional Speech.\n","metadata":{}},{"cell_type":"markdown","source":"## Setup & Libraries","metadata":{}},{"cell_type":"code","source":"!pip install noisereduce\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T04:24:17.157762Z","iopub.execute_input":"2025-07-15T04:24:17.158176Z","iopub.status.idle":"2025-07-15T04:24:28.724213Z","shell.execute_reply.started":"2025-07-15T04:24:17.158118Z","shell.execute_reply":"2025-07-15T04:24:28.723121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport librosa\nimport librosa.display\nimport soundfile as sf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport noisereduce as nr\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\nemotion_map = {\n    'W': 'anger',\n    'L': 'boredom',\n    'A': 'anxiety',\n    'F': 'happiness',\n    'T': 'sadness',\n    'E': 'disgust',\n    'N': 'neutral'\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T04:24:28.726386Z","iopub.execute_input":"2025-07-15T04:24:28.726720Z","iopub.status.idle":"2025-07-15T04:24:32.671606Z","shell.execute_reply.started":"2025-07-15T04:24:28.726687Z","shell.execute_reply":"2025-07-15T04:24:32.670525Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Audio Processing Functions\nFunctions for loading, modifying, and saving audio.\n","metadata":{}},{"cell_type":"code","source":"# Function to load audio\ndef load_audio(file_path):\n    y, sr = librosa.load(file_path, sr=None)\n    return y, sr\n\n# Change speed using phase vocoder\ndef change_speed(audio, rate):\n    hop_length = 512\n    stft = librosa.stft(audio, hop_length=hop_length)\n    stft_stretched = librosa.phase_vocoder(stft, rate=rate, hop_length=hop_length)\n    audio_stretched = librosa.istft(stft_stretched, hop_length=hop_length)\n    return audio_stretched\n    \n# Change pitch\ndef change_pitch(audio, sr, n_steps):\n    return librosa.effects.pitch_shift(audio, sr=sr, n_steps=n_steps)\n\n# Noise reduction using sample noise\ndef reduce_noise(audio, sr, noise_sample):\n    return nr.reduce_noise(y=audio, sr=sr, y_noise=noise_sample)\n    \n# Equalize audio\ndef equalize_audio(audio):\n    return librosa.effects.preemphasis(audio)\n    \n# Save audio\ndef save_audio(file_path, audio, sr):\n    sf.write(file_path, audio, sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T04:26:44.900012Z","iopub.execute_input":"2025-07-15T04:26:44.900428Z","iopub.status.idle":"2025-07-15T04:26:44.908469Z","shell.execute_reply.started":"2025-07-15T04:26:44.900393Z","shell.execute_reply":"2025-07-15T04:26:44.907352Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Visualization\nFunctions to plot waveforms and FFTs.\n","metadata":{}},{"cell_type":"code","source":"\n\n# Plot waveform\ndef plot_waveform(audio, sr, title=\"Waveform\"):\n    plt.figure(figsize=(10, 4))\n    librosa.display.waveshow(audio, sr=sr)\n    plt.title(title)\n    plt.xlabel(\"Time (s)\")\n    plt.ylabel(\"Amplitude\")\n    plt.tight_layout()\n    plt.show()\n\n# Plot FFT\ndef plot_fft(audio, sr, title=\"FFT of Audio\"):\n    fft_audio = np.fft.fft(audio)\n    fft_freq = np.fft.fftfreq(len(audio), 1 / sr)\n    positive_freqs = fft_freq[:len(fft_freq)//2]\n    positive_fft_audio = np.abs(fft_audio[:len(fft_audio)//2])\n    plt.figure(figsize=(10, 4))\n    plt.plot(positive_freqs, positive_fft_audio)\n    plt.title(title)\n    plt.xlabel(\"Frequency (Hz)\")\n    plt.ylabel(\"Magnitude\")\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T04:26:46.249156Z","iopub.execute_input":"2025-07-15T04:26:46.249562Z","iopub.status.idle":"2025-07-15T04:26:46.257462Z","shell.execute_reply.started":"2025-07-15T04:26:46.249522Z","shell.execute_reply":"2025-07-15T04:26:46.256367Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Feature Extraction\n\nExtracting MFCCs, pitch, and energy from audio samples.\n","metadata":{}},{"cell_type":"code","source":"def extract_features(audio_folder):\n    features = []\n    labels = []\n    combined_mfcc = None\n    combined_pitch = None\n    combined_energy = None\n\n    for file_name in os.listdir(audio_folder):\n        if file_name.endswith('.wav'):\n            audio_path = os.path.join(audio_folder, file_name)\n            emotion_code = file_name[5]  \n            emotion_label = emotion_map.get(emotion_code)\n\n            if emotion_label: \n                try:\n                    y, sr = librosa.load(audio_path, sr=16000)\n                    \n                    # Extract MFCC\n                    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n                    if combined_mfcc is None:\n                        combined_mfcc = mfcc\n                    else:\n                        combined_mfcc = np.hstack((combined_mfcc, mfcc))\n\n                    # Extract Pitch\n                    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n                    pitch_mean = np.mean(pitches[pitches > 0])\n                    if combined_pitch is None:\n                        combined_pitch = np.array([pitch_mean])\n                    else:\n                        combined_pitch = np.hstack((combined_pitch, np.array([pitch_mean])))\n\n                    # Extract Energy\n                    energy = np.mean(librosa.feature.rms(y=y))\n                    if combined_energy is None:\n                        combined_energy = np.array([energy])\n                    else:\n                        combined_energy = np.hstack((combined_energy, np.array([energy])))\n\n                    # Combine Features\n                    feature_vector = np.hstack((np.mean(mfcc, axis=1), pitch_mean, energy))\n                    features.append(feature_vector)\n                    labels.append(emotion_label)\n\n                except Exception as e:\n                    print(f\"Error processing {file_name}: {e}\")\n    return np.array(features), np.array(labels), combined_mfcc, combined_pitch, combined_energy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T04:26:48.776539Z","iopub.execute_input":"2025-07-15T04:26:48.776904Z","iopub.status.idle":"2025-07-15T04:26:48.787115Z","shell.execute_reply.started":"2025-07-15T04:26:48.776874Z","shell.execute_reply":"2025-07-15T04:26:48.785931Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Audio Preprocessing and Visualization\nApply transformations and visualize their impact.\n","metadata":{}},{"cell_type":"code","source":"# Main Processing Pipeline\ndef process_audio_files(dataset_path, output_path):\n    if not os.path.exists(output_path):\n        os.makedirs(output_path)\n\n    features, labels, combined_mfcc, combined_pitch, combined_energy = extract_features(dataset_path)\n    audio_files = [f for f in os.listdir(dataset_path) if f.endswith('.wav')] \n\n    # Process only the first audio file\n    for i, file_name in enumerate(audio_files):\n        if i == 1:  # Stop after processing the first audio file\n            break\n        \n        file_path = os.path.join(dataset_path, file_name)\n        y, sr = load_audio(file_path)\n        \n        print(f\"Processing file: {file_name}\")\n        \n        # Plot waveform before FFT\n        plot_waveform(y, sr, title=f\"Original Audio - {file_name}\")\n        \n        # Plot FFT before applying changes\n        plot_fft(y, sr, title=f\"Original FFT - {file_name}\")\n\n        # Time Stretching before and FFT after processing\n        y_stretched = change_speed(y, rate=1.5)\n        save_audio(os.path.join(output_path, f\"stretched_{file_name}\"), y_stretched, sr)\n        plot_waveform(y_stretched, sr, title=f\"Time-Stretched Audio - {file_name}\")\n        plot_fft(y_stretched, sr, title=f\"Time-Stretched FFT - {file_name}\")\n\n        # Pitch Shifting before and FFT after processing\n        y_pitched = change_pitch(y, sr, n_steps=4)\n        save_audio(os.path.join(output_path, f\"pitched_{file_name}\"), y_pitched, sr)\n        plot_waveform(y_pitched, sr, title=f\"Pitch-Shifted Audio - {file_name}\")\n        plot_fft(y_pitched, sr, title=f\"Pitch-Shifted FFT - {file_name}\")\n\n        # Equalization before and FFT after processing\n        y_equalized = equalize_audio(y)\n        save_audio(os.path.join(output_path, f\"equalized_{file_name}\"), y_equalized, sr)\n        plot_waveform(y_equalized, sr, title=f\"Equalized Audio - {file_name}\")\n        plot_fft(y_equalized, sr, title=f\"Equalized FFT - {file_name}\")\n\n        # Noise Reduction before and FFT after processing\n        noise_sample = y[:len(y)//10]\n        y_denoised = reduce_noise(y, sr, noise_sample)\n        save_audio(os.path.join(output_path, f\"denoised_{file_name}\"), y_denoised, sr)\n        plot_waveform(y_denoised, sr, title=f\"Denoised Audio - {file_name}\")\n        plot_fft(y_denoised, sr, title=f\"Denoised FFT - {file_name}\")\n\n        # Plot Combined MFCC for All Files\n        plt.figure(figsize=(10, 4))\n        librosa.display.specshow(combined_mfcc, x_axis='time')\n        plt.colorbar(format=\"%+2.0f dB\")\n        plt.title('MFCCs for All Audio Files')\n        plt.tight_layout()\n        plt.show()\n    \n        # Plot Combined Pitch for All Files\n        plt.figure(figsize=(10, 4))\n        plt.plot(combined_pitch)\n        plt.title('Pitch for All Audio Files')\n        plt.xlabel('Time')\n        plt.ylabel('Pitch')\n        plt.tight_layout()\n        plt.show()\n        \n        # Plot Combined Energy for All Files\n        plt.figure(figsize=(10, 4))\n        plt.plot(combined_energy)\n        plt.title('Energy for All Audio Files')\n        plt.xlabel('Time')\n        plt.ylabel('Energy')\n        plt.tight_layout()\n        plt.show()\n    \n        return features, labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T04:26:51.671843Z","iopub.execute_input":"2025-07-15T04:26:51.672780Z","iopub.status.idle":"2025-07-15T04:26:51.686503Z","shell.execute_reply.started":"2025-07-15T04:26:51.672739Z","shell.execute_reply":"2025-07-15T04:26:51.685322Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Model Training\nTrain a neural network classifier (MLP) on extracted features.\n","metadata":{}},{"cell_type":"code","source":"# Train Emotion Classification Model\ndef train_emotion_model(features, labels):\n    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    label_encoder = LabelEncoder()\n    y_train = label_encoder.fit_transform(y_train)\n    y_test = label_encoder.transform(y_test)\n\n    model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    print(\" MLP Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100) )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T04:26:55.347034Z","iopub.execute_input":"2025-07-15T04:26:55.347525Z","iopub.status.idle":"2025-07-15T04:26:55.356310Z","shell.execute_reply.started":"2025-07-15T04:26:55.347483Z","shell.execute_reply":"2025-07-15T04:26:55.354533Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##  Run the Full Pipeline\nCall functions and observe the results.\n","metadata":{}},{"cell_type":"code","source":"\n# Paths\ndataset_path = \"/kaggle/input/berlin-database-of-emotional-speech-emodb/wav\"   \noutput_path = \"processed_audio\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T04:27:06.828679Z","iopub.execute_input":"2025-07-15T04:27:06.829102Z","iopub.status.idle":"2025-07-15T04:27:06.833675Z","shell.execute_reply.started":"2025-07-15T04:27:06.829065Z","shell.execute_reply":"2025-07-15T04:27:06.832681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Run the Pipeline\nfeatures, labels = process_audio_files(dataset_path, output_path)\ntrain_emotion_model(features, labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T04:27:10.502154Z","iopub.execute_input":"2025-07-15T04:27:10.502650Z","iopub.status.idle":"2025-07-15T04:27:34.250722Z","shell.execute_reply.started":"2025-07-15T04:27:10.502607Z","shell.execute_reply":"2025-07-15T04:27:34.247234Z"}},"outputs":[],"execution_count":null}]}